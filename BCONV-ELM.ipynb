{"cells":[{"metadata":{},"cell_type":"markdown","source":"The following cells implement modules to train a convolutional neural network with binary kernels (for resources-constrained platforms). The final model is available in fourth cell. This software is companion to paper: \nR. Dogaru and Ioana Dogaru, \"BCONV-ELM: Binary Weights Convolutional \nNeural Network Simulator based on Keras/Tensorflow for Low Complexity Implementations\", Proceedings of ISEEE 2019, in Press. \n\nPlease cite the above paper if you find this code useful \n- First module (BCONV) implements a 2-layer convolution expansion filter and applies it on a selected dataset providing the output tensors out_train and out_test which can be further used as inputs in additional layers (a linear adaptive unit or MLP0 is recommended for low complexity implements) \n- Second module (ELM)implements an Extreme Learning Machine and it is a revised version using Keras backend operators (thus supporting GPU) of the simulator available in https://github.com/radu-dogaru/ELM-super-fast  With its  0-neurons (no hidden layer) options it is used to optimize the randomly generated binary convolution kernels in BCONV for best performance. \n- Third module (MLP) implements a Keras multi-layer perceptron as output stage of the network (MLP0 - i.e. no hidden layer) is recommended for compact models. \n- Fourth module (QUANT) ensures the 8 bit fixed point quantization of the MLP0 weights and concludes the model as required by a resources-constrained implementation. \n\nCopyright Radu & Ioana DOGARU - radu_d@ieee.org , Aug. 13 2019  \nLicense: https://choosealicense.com/licenses/bsd-3-clause/\nNotice: On Kaggle platform please check to have Internet = ON GPU = ON (CPU works, but slowly)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"'''\nBCONV MODULE \nThis module loads dataset (one of MNIST, CIFAR10 or Fashion-MNIST)\n# Copyright Radu & Ioana DOGARU - radu_d@ieee.org , Aug. 13 2019 \n# More details in paper \n# R. Dogaru and Ioana Dogaru, \"BCONV-ELM: Binary Weights Convolutional \n# Neural Network Simulator based on Keras/Tensorflow for Low Complexity \n# Implementations\", in Proceedings ISEEE 2019, in Press. \n# Please cite the above paper if you find this code useful \n'''\nimport keras.backend as K\nimport tensorflow as tf\nimport scipy.linalg as sclin\nimport numpy as np\nimport time as ti\nfrom sklearn.preprocessing import StandardScaler\n'''\nUses K.backend methods to implement (GPU/CPU) a preprocessing \nlayer with up to 2 convolutional layers (each with abs() \nvalue nonlinearity and pooling)\nConvolution kernels are BINARY and RANDOM !\nTHIS MODULE WOULD GENERATE out_train out_test \nAS THE OUTPUT OF A PREPROCESSING NONLINEAR LAYER \nWITH UP TO 2 CONVOLUTIONAL LAYERS AND x_train x_test tuple at the input. \n'''\n\n#------------------------ Algo parameters ----------\n########################################################\ndataset='mnist' # or f-mnist or cifar10 \n# convolutional layers \ntyp_con=1 #  1 = depthwise (); 2 = normal (as usually used in Keras CNNs)\nfiltre1=10; filtre2=16 # if filtre2=0 only the first layer is implemented  \nksize1=3; ksize2=3    # kernel size on layers 1 & 2 \npool=(2,2); pool2=(3,3)  # pool size on layers 1 & 2 \nstrp1=2; strp2=2 # strides on pooling layers 1 & 2 \nstrc1=1; strc2=1 # strides on conv layers 1 & 2 \npmode='max'  # pooling mode (alternative is  'average', lower performance) \npad='same' # 'same' (alternative is  'valid' ) results in less outputs but performance may decrease\nreduced = 10000 # 0-full; >0 = reduced set of training samples (using all samples - choose 0)\nuse_stored = 0 # use bk wher bk=(ker,ker2) are best kernels (saved after run in console mode)\n# choose 0 above for loop tuning (then run all cells 5-10 trials saving bk=(ker,ker2) for any improvement in ELM0 accuracy)\n# choose 1 above after tuning then run all cells (now ELM is not active)\nepoci=1 # epochs in the MLP (use 1 in the ELM loop tuning phase with Run ALL)\n\nprint('Dataset is:', dataset)\nprint('Convolution type is (1-depthwise, 2-classic)',typ_con)\nprint('Layer 1: filters:',filtre1,' kernel size:',ksize1,'pooling size:',pool,'strides pooling:',strp1)\nprint('Layer 1: filters:',filtre2,' kernel size:',ksize2,'pooling size:',pool2,'strides pooling:',strp2)\nprint('Pooling mode is:',pmode,'  Padding is: ',pad)\n\n# definitie primul strat de convolutii (liniare)\ndef convlayer(inlay,ker_,pool,strp,strc,typ_con,pad,pmode):\n# inlay este intrarea (multicanal si organizata [samples, x_size, y_size, channells])\n# ker_ este variabila kernel organizata in forma [x_size, x_sixe, channels, filtre ]\n    dformat='channels_last'\n    if typ_con==1:\n        out_=K.depthwise_conv2d(inlay, ker_, strides=(1, 1), padding=pad, data_format=dformat)\n    elif typ_con==2:\n        out_=K.conv2d(inlay, ker_, strides=(1, 1), padding=pad, data_format=dformat)\n        \n    nout_=K.pool2d(out_,pool, strides=(strp, strp), padding=pad, data_format=dformat ,pool_mode=pmode)\n    nout__=K.abs(nout_) # useful nonlinearity   (instead of  RELU)\n    out_=K.batch_flatten(nout__)  # flatten all ELM \n    return out_, nout_\n\n# Datasets import (Internet must be ON)\n# mnist, cifar have inputs integers up to 255 \nfrom keras.datasets import mnist, cifar10, fashion_mnist\n\nif dataset=='mnist':\n    (x_train, y_train), (x_test, y_test) = mnist.load_data() # incarca date nescalate \nelif  dataset=='cifar10': \n    (x_train, y_train), (x_test, y_test) = cifar10.load_data() # incarca date nescalate \nelif dataset=='f-mnist':\n    (x_train, y_train), (x_test, y_test) =  fashion_mnist.load_data()\n\nif (np.ndim(x_train)==3):   # E.g.  MNIST or F-MNIST  \n    x_train=np.reshape(x_train, [np.shape(x_train)[0],np.shape(x_train)[1],np.shape(x_train)[2], 1]) \n    x_test=np.reshape(x_test, [np.shape(x_test)[0],np.shape(x_test)[1],np.shape(x_test)[2], 1] ) \n# place a  1 in the end to keep it compatible with kernel in conv2d \n# scaling in ([0,1])\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /=255 \n\n# one can choose a lower numbers of training samples (when GPU MEM is overloaded)\nif reduced>0:\n    Ntr1=reduced\n    x_train=x_train[0:Ntr1,:,:,:]\n    y_train=y_train[0:Ntr1]\n\n#================ Apply the conv. layers ==============================\ninp_chan=np.shape(x_train)[3] \nprint('Number of input channels in image:', inp_chan)\n\n# Construct model (binary random kernels) \n# Layer 1 kernels (without bias)\nker=np.sign(np.random.rand(ksize1,ksize1,inp_chan,filtre1).astype('float32')-0.5)\n#ker=(np.random.rand(ksize1,ksize1,inp_chan,filtre1).astype('float32')-0.5)\nker=1*(ker)/(ksize1*ksize1) #scaling \n\nif use_stored==1: \n    ker=bk[0]\n# Layer 2 kernels (without bias)\nif typ_con==1:\n    ker2=np.sign(np.random.rand(ksize2,ksize2,filtre1*inp_chan,filtre2).astype('float32')-0.5)\n    #ker2=np.random.rand(ksize2,ksize2,filtre1*inp_chan,filtre2).astype('float32')-0.5\n\nelif typ_con==2: \n    ker2=np.sign(np.random.rand(ksize2,ksize2,filtre1,filtre2).astype('float32')-0.5)\n    # scaling \n    #ker2=1*(ker2)/(ksize2*ksize2)\n    #ker2=bker2\n    #There are 4 dimensions  (a,b,c,d) a,b dim. kernel ; \n    # c -numeber of channels (1 or 3 for the input, more for 2'nd layer) \n    # d is the number of  filters opened by this convolution layer .. \nif use_stored==1:\n    ker2=bk[1]    \n# Implementing the processing flow for (train / test) \nintrain=K.variable(x_train)\nintest=K.variable(x_test)\nker_=K.variable(ker)\nker2_=K.variable(ker2)\n\nout_train1_, nout_tr=convlayer(intrain,ker_,pool,strp1,strc1,typ_con,pad,pmode)\nout_test1_, nout_ts=convlayer(intest,ker_,pool,strp1,strc1,typ_con,pad,pmode)\n# Note: out_ are \"flatten\" outputs,  nout_ are structured outputs  \nprint('Layer 1:',np.shape(nout_tr))\n# Plus an additional conv layer  \nif filtre2>0: \n    out_train_, nout_tr2=convlayer(nout_tr,ker2_,pool2,strp2,strc2,typ_con,pad,pmode)\n    out_test_, nout_ts2=convlayer(nout_ts,ker2_,pool2,strp2,strc2,typ_con,pad,pmode) \n    print('Layer 2:',np.shape(nout_tr2))\nelse:\n    out_train_=out_train1_\n    out_test_=out_test1_\n    \n# Effective computation of the input preprocessing flow \nt1=ti.time()\nout_train=K.eval(out_train_)\nout_test=K.eval(out_test_)\nt2=ti.time()\nK.clear_session()  # to avoid overloads \n\n# Number of parameters \n\n\nprint('Convolutional layers processing time (train + test): ',t2-t1)\nprint('Number of bits in KER1:',np.size(ker))\nprint('Number of bits in KER2:',np.size(ker2))\nprint('Output structure:  ',np.shape(out_train))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be8e22732be009d79c4e90d6489bf72a0543333b"},"cell_type":"code","source":"'''\n#  ELM MODULE  \n#================================================================================\n#  Due to limited GPU RAM - input size should be smaller than 5000 on Kaggle \n#  or arrange to have less input samples (e.g. 10000 input samples)\n#  On other platforms it depends on the available resources. \n#  For larger inputs size is better to use the latest cell (implementing a\n#  typical multilayer perceptron  MLP in Keras)\n# Copyright Radu & Ioana DOGARU - radu_d@ieee.org \n# More details in paper \n# R. Dogaru and Ioana Dogaru, \"BCONV-ELM: Binary Weights Convolutional \n# Neural Network Simulator based on Keras/Tensorflow for Low Complexity \n# Implementations\", in Proceedings ISEEE 2019, in Press. \n# Please cite the above paper if you find this code useful \n# \n#--------------------------------------------------------------------------\n'''\n\nnr_neuroni=0 # Proposed number of neurons on the hidden layer \nC=100 # Regularization coefficient C  (small value / useful for 0 neurons)\nif nr_neuroni==0:\n    C=0.1 # only for no-hidden layer \ntip=3 # Nonlinearity of the hidden layer (-1 means linear layer)\nif nr_neuroni==0:\n    tip=-1   # \nnb_in=2;  # 0 = float; x - represents weights on a finite x number of bits \nnb_out=8; # same as above but for the output layer\norig=0 # 0 - works with (out_train out_test) from BCONV; \n       # 1 works with original data - no conv. layers (x_train x_test)\n       # 0 - convolved data ; 1 - directly applied to ELM \n#===============  TRAIN DATASET LOADING ==========================================\n\ndef hidden(x_,inw_,tip):\n# Hidden layer definit ca \"flow\" Keras (argumentele sunt \"variables\")\n      hin_=K.dot(inw_,x_)\n      #----------  HIDDEN LAYER --------- \n      if tip==-1:  # liniar (Adaline only)\n        h_=hin_\n      elif tip==0: # tanh\n        h_=K.tanh(hin_)\n      elif tip==1:  # linsat \n        h_=K.abs(1+hin_)-K.abs(1-hin_)\n      elif tip==2: # ReLU\n        h_=K.relu(hin_)\n      elif tip==3: \n            h_=K.abs(hin_)\n      elif tip==4:\n            h_=K.sqrt(K.square(hin_)+1)\n      #------------------------------------ \n      return h_\n\n# implements the ELM training procedure with weight quantization       \ndef elmTrain_fix( X, Y, h_Neurons, C , tip, ni):\n# Training phase - emulated fixed point precision (ni bit quantization)\n# X - Samples (feature vectors) Y - Labels\n# ni - number of bits to quantize the inW weights \n      Ntr = np.size(X,1)\n      in_Neurons = np.size(X,0)\n      classes = np.max(Y)\n      # transforms label into binary columns  \n      targets = np.zeros( (classes, Ntr), dtype='int8' )\n      for i in range(0,Ntr):\n          targets[Y[i]-1, i ] = 1\n      targets = targets * 2 - 1\n      \n      #   Generare inW \n      #   Generate inW layer \n      #   Takes care if h_Neurons==0 \n      if h_Neurons==0:\n          inW=np.eye(in_Neurons)\n          h_Neurons=in_Neurons\n    \n      else: \n          rnd = np.random.RandomState()\n          inW=-1+2*rnd.rand(h_Neurons, in_Neurons).astype('float32')\n          #inW=rnd.randn(nHiddenNeurons, nInputNeurons).astype('float32')\n          if ni>0:\n            Qi=-1+pow(2,ni-1) \n            inW=np.round(inW*Qi)\n      \n      #  Compute hidden layer \n      iw_=K.variable(inW)\n      x_=K.variable(X)\n      h_=hidden(x_,iw_,tip)  \n      #------------------------------------      \n      # Moore - Penrose computation of output weights (outW) layer \n      ta_=K.variable(targets)\n      print('KERAS ACTIVE')\n      if h_Neurons<Ntr:\n          print('LLL - Less neurons than training samples')\n          outw_=tf.matrix_solve(K.eye(h_Neurons)/C+K.dot(h_,K.transpose(h_)),K.dot(h_,K.transpose(ta_)))  \n      else:\n          print('MMM - More neurons than training samples')\n          outw_=K.dot(h_,tf.matrix_solve(K.eye(Ntr)/C+K.dot(K.transpose(h_),h_),K.transpose(ta_)))\n      outW=K.eval(outw_)   \n      K.clear_session()     \n      return inW, outW \n      \n\ndef elmPredict_optim( X, inW, outW, tip):\n# implements the ELM predictor given the model as arguments \n# model is simply given by inW, outW and tip \n# returns a score matrix (winner class has the maximal score)\n      x_=K.variable(X)\n      iw_=K.variable(inW)\n      ow_=K.variable(outW)\n      h_=hidden(x_,iw_,tip) \n      mul1=K.dot(K.transpose(h_),ow_)\n      sc_=K.transpose(mul1)\n      score = K.eval(sc_)\n      K.clear_session() \n      return score \n        \n\n#===============  TRAIN DATASET LOADING ==========================================\n# converts out_train, y_train into Samples Labels \n\nif orig==1:\n    intrain=K.variable(x_train)\n    Samples_=K.batch_flatten(intrain)  # aici se aplica direct datele de intrare \n    Samples=(K.eval(Samples_)).T\nelse:\n    Samples=out_train.T \n\nLabels=(y_train.T+1).astype('int8')\nif (np.ndim(Labels)<2): \n    Labels=np.reshape(Labels,[1,np.shape(Labels)[0]])\nclase=np.max(Labels)\n#================= TRAIN ELM =====================================================\nt1 = ti.time()\ninW, outW = elmTrain_fix(Samples, np.transpose(Labels), nr_neuroni, C, tip, nb_in)\ntrun = ti.time()-t1\nprint(\" training time: %f seconds\" %trun)\n\n# ==============  Quantify the output layer ======================================\nQout=-1+pow(2,nb_out-1)\nif nb_out>0:\n     O=np.max(np.abs(outW))\n     outW=np.round(outW*(1/O)*Qout)\n\n#================= TEST (VALIDATION) DATASET LOADING \n\n\nif orig==1:\n    intest=K.variable(x_test)\n    Samples_=K.batch_flatten(intest)  # aici se aplica direct datele de intrare \n    Samples=(K.eval(Samples_)).T\nelse: \n    Samples=out_test.T \n\nLabels=(y_test.T+1).astype('int8')\nif (np.ndim(Labels)<2):\n    Labels=np.reshape(Labels,[1,np.shape(Labels)[0]])  # acopera cazul MNIST \n\nn=Samples.shape[0]\nN=Samples.shape[1]\n\n#====================== VALIDATION PHASE (+ Accuracy evaluation) =================\nt1 = ti.time()\nscores = elmPredict_optim(Samples, inW, outW, tip)\ntrun = ti.time()-t1\nprint( \" prediction time: %f seconds\" %trun)\n\n# CONFUSION MATRIX computation ==================================\nConf=np.zeros((clase,clase),dtype='int16')\nfor i in range(N):\n    # gasire pozitie clasa prezisa \n    ix=np.where(scores[:,i]==np.max(scores[:,i]))\n    ixx=np.array(ix)\n    pred=int(ixx[0,0])\n    actual=Labels[0,i]-1\n    Conf[actual,pred]+=1\naccuracy=100.0*np.sum(np.diag(Conf))/np.sum(np.sum(Conf))\nprint(\"Confusion matrix is: \")\nprint(Conf)\nprint(\"Accuracy is: %f\" %accuracy)\nprint( \"Number of hidden neurons: %d\" %nr_neuroni)\nprint( \"Hidden nonlinearity (0=sigmoid; 1=linsat; 2=Relu; 3 - ABS; 4- multiquadric): %d\" %tip)\nK.clear_session() \n#====================================================================================   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -----------------------------------------------------------------------\n# KERAS MLP (Up to 2 hidden layer - 0 hidden layers (MLP0)= Adaline \n#  is recommended for low cpx.) \n#  Slower than  ELM0, but gives better accuracies\n#  Fixed point quantization of the MLP0 is implemented in the last cell !\n#--------------------------------------------------------\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import RMSprop, SGD, Adadelta, Adam, Nadam \n\nimport matplotlib.pyplot as plt\n\nbatch_size = 100  #  (small values - larger training times but better accuraccy ) \n#epoci = 100 # Number of epochs \ntrain_style = 2 # 1-the Keras usual in examples ; 2 keeps best result during training in all epochs (recommended)\n#--------------  structura model neuronal --------------------------------------------------------\nnescalat =0  # 0 for scaled data (1 additional scalling) \nnhid1 = 0 # hidden1 - take 0 for MLP0 (Adaline) - if > 0 nhid2 may also be used \nnhid2 = 0 # hidden2 - take 0 for MLP0 (or other value if nhid1>0)\ndrop1= 0.0 # droput if drop1>0\n# Liniar output layer (nhid1=nhid2=0)\n# ------------------- Optimizer -----------------------------------------------------------------\n#myopt=SGD(lr=0.01)\n#myopt = SGD(lr=.007, decay=1e-6, momentum=.9, nesterov=True)\nmyopt =Adadelta(lr=0.1)  # implicit are lr=1 # cum influenteaza valoarea procesul de antrenare ?? \n#myopt = RMSprop(lr=0.01) \n#myopt = Nadam()\n#myopt=Adam()\n\n# -------------------------- Loss choice ------------------------------------\n#my_loss='mean_squared_error'  # desigur se pot alege si alte versiuni \n#my_loss='mean_absolute_error'\nmy_loss='categorical_crossentropy'\n\n# ---------------  conversie  categorical (vector coloana cu 1 pe linia asociata clasei) \nnum_classes = 1+np.max(y_test).astype('int8')\nnum_inputs = np.shape(out_test)[1]\nprint('Number of inputs: ',num_inputs)\nyc_train = keras.utils.to_categorical(y_train, num_classes)\nyc_test = keras.utils.to_categorical(y_test, num_classes)\n\n#------------  MODEL Definition  ----------------------------------------\n# Nota: se poate construi propriul model (vezi https://gist.github.com/abhaikollara/430c0491c851cf0b05a852f1faa805d7  )\n# ----------------------------------------------\nmodel = Sequential()\nif nhid1>0:\n    model.add(Dense(nhid1, activation='relu', input_shape=(num_inputs,)))\n    if drop1>0: \n        model.add(Dropout(drop1))\n# second hidden layer \nif nhid2>0:\n    model.add(Dense(nhid2, activation='relu'))\n#   model.add(Dropout(0.2))\n# output layer \nif (nhid1+nhid2)==0:\n    model.add(Dense(num_classes, activation='softmax',input_shape=(num_inputs,)))\nelse: \n    model.add(Dense(num_classes, activation='softmax'))\n\n\n# --- MODEL COMPILE ---------------------------------------\n\nmodel.compile(loss=my_loss, \n              optimizer=myopt,   \n              metrics=['accuracy'])\n\n# --- RUNS THE MODEL \n#----\nmodel.summary()\n\nerr_test=np.zeros(epoci)   # For plot   \nbest_acc=0.0\nbest_ep=0\nt1=ti.time()\nfor k in range(epoci):\n    model.fit(out_train, yc_train,\n              batch_size=batch_size,\n              epochs=1,\n              verbose=0,  #  0 (no details) 1 (details) 2(only epochs)\n              validation_data=(out_test, yc_test))\n    score = model.evaluate(out_test, yc_test, verbose=0)\n    err_test[k]=score[1]\n    print('Epoch:',k,' ACC:',100*score[1],'| ')\n    if score[1]>best_acc : \n            print('improved in epoch:', k, 'best accuracy so far: ', 100*score[1],'%')\n            best_acc=score[1]\n            best_ep=k\n            best_pars=model.get_weights()\nt2=ti.time()\nprint('Best accuracy:', best_acc*100, '% obtained in epoch: ',best_ep, ' running  ',epoci,' epochs in ',t2-t1,' seconds')\nt1=ti.time()\nscore = model.evaluate(out_test, yc_test, verbose=0); \nt2=ti.time()\nprint('Time for prediction (test set):', t2-t1)\nplt.plot(err_test)\nprint('If quantization is desired see the next cell')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# QMLP0 (quantified MLP0 module)\n# Quantization of the above resulted model (only for MLP0)\n# Copyright Radu and Ioana DOGARU;  radu.dogaru@ieee.org\n#=============================================================\nnb_out=8\noutW=np.copy(best_pars)\nQout=-1+pow(2,nb_out-1)\nif (nb_out >0) & (nhid1==0) :\n    O=np.max(np.abs(outW[0]))\n    outW[0]=np.round(outW[0]*(1/O)*Qout)\n    outW[1]=np.round(outW[1]*(1/O)*Qout)\n    model.set_weights(outW)\n    score = model.evaluate(out_test, yc_test, verbose=0)\n    best_acc=score[1]\n    print('Output layer quantized with:', nb_out, 'bits')\n    print('Quantified accuracy is:', best_acc*100,'%')\noutW=model.get_weights() # the resulting model \nBconvmodel=(ker,ker2,outW)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}