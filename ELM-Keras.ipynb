{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"'''\n#  ELM MODULE  \n#================================================================================\n#  Due to limited GPU RAM - input size should be smaller than 5000 on Kaggle \n#  On other platforms it depends on the available resources. \n#  For larger inputs size is better to use the latest cell (implementing a\n#  typical multilayer perceptron in Keras)\n# Copyright Radu & Ioana DOGARU - radu_d@ieee.org \n# More details in paper \n# R. Dogaru and Ioana Dogaru, \"BCONV-ELM: Binary Weights Convolutional \n# Neural Network Simulator based on Keras/Tensorflow for Low Complexity \n# Implementations\", in Proceedings ISEEE 2019, in Press. \n# Please cite the above paper if you find this code useful \n# \n#--------------------------------------------------------------------------\n'''\nimport keras.backend as K\nimport tensorflow as tf\nimport scipy.linalg as sclin\nimport numpy as np\nimport time as ti\nimport scipy.io as sio\n# ------------------------  ELM module parameters -------------\ndataset='f-mnist' # it can be mnist, cifar10, f-mnist, other loads a local file\nifother='...' # depends on your available files (they should be in .mat format)\nnr_neuroni=6000 # Proposed number of neurons on the hidden layer \n#C=0.100000 # Regularization coefficient C  (small value / useful for 0 neurons)\nC=10 # Useful in the case of a hidden layer with N>>0 neurons \ntip=3 # Nonlinearity of the hidden layer (-1 means linear layer)\nif nr_neuroni==0:\n    tip=-1   # \nnb_in=2;  # 0 = float; x - represents weights on a finite x number of bits \nnb_out=8; # same as above but for the output layer\nfirst_samples=40000; # 0 - all samples ; [value] - first [value] samples\n\n# =============  ELM basic functions \ndef hidden(x_,inw_,tip):\n# Hidden layer definit ca \"flow\" Keras (argumentele sunt \"variables\")\n      hin_=K.dot(inw_,x_)\n      #----------  HIDDEN LAYER --------- \n      if tip==-1:  # liniar (pentru Adaline only)\n        h_=hin_\n      elif tip==0: # tanh\n        h_=K.tanh(hin_)\n      elif tip==1:  # linsat \n        h_=K.abs(1+hin_)-K.abs(1-hin_)\n        # de verificat daca 1+ merge ... \n      elif tip==2: # ReLU\n        h_=K.relu(hin_)\n      elif tip==3: \n            h_=K.abs(hin_)\n      elif tip==4:\n            h_=K.sqrt(K.square(hin_)+1)\n      #------------------------------------ \n      return h_\n\n# implements the ELM training procedure with weight quantization       \ndef elmTrain_fix( X, Y, h_Neurons, C , tip, ni):\n# Training phase - emulated fixed point precision (ni bit quantization)\n# X - Samples (feature vectors) Y - Labels\n# ni - number of bits to quantize the inW weights \n      Ntr = np.size(X,1)\n      in_Neurons = np.size(X,0)\n      classes = np.max(Y)\n      # transforms label into binary columns  \n      targets = np.zeros( (classes, Ntr), dtype='int8' )\n      for i in range(0,Ntr):\n          targets[Y[i]-1, i ] = 1\n      targets = targets * 2 - 1\n      \n      #   Generare inW \n      #   Generate inW layer \n      #   Takes care if h_Neurons==0 \n      if h_Neurons==0:\n          inW=np.eye(in_Neurons)\n          h_Neurons=in_Neurons\n    \n      else: \n          rnd = np.random.RandomState()\n          inW=-1+2*rnd.rand(h_Neurons, in_Neurons).astype('float32')\n          #inW=rnd.randn(nHiddenNeurons, nInputNeurons).astype('float32')\n          if ni>0:\n            Qi=-1+pow(2,ni-1) \n            inW=np.round(inW*Qi)\n      \n      #  Compute hidden layer \n      iw_=K.variable(inW)\n      x_=K.variable(X)\n      h_=hidden(x_,iw_,tip)  \n      #------------------------------------      \n      # Moore - Penrose computation of output weights (outW) layer \n      ta_=K.variable(targets)\n      print('KERAS ACTIVE')\n      if h_Neurons<Ntr:\n          print('LLL - Less neurons than training samples')\n          outw_=tf.matrix_solve(K.eye(h_Neurons)/C+K.dot(h_,K.transpose(h_)),K.dot(h_,K.transpose(ta_)))  \n      else:\n          print('MMM - More neurons than training samples')\n          outw_=K.dot(h_,tf.matrix_solve(K.eye(Ntr)/C+K.dot(K.transpose(h_),h_),K.transpose(ta_)))\n      outW=K.eval(outw_)   \n      K.clear_session()     \n      return inW, outW \n      \n\ndef elmPredict_optim( X, inW, outW, tip):\n# implements the ELM predictor given the model as arguments \n# model is simply given by inW, outW and tip \n# returns a score matrix (winner class has the maximal score)\n      x_=K.variable(X)\n      iw_=K.variable(inW)\n      ow_=K.variable(outW)\n      h_=hidden(x_,iw_,tip) \n      mul1=K.dot(K.transpose(h_),ow_)\n      sc_=K.transpose(mul1)\n      score = K.eval(sc_)\n      K.clear_session() \n      return score \n        \ndef read_mat_data(nume):\n    # reads data saved in the LIBSVM  .mat format (Samples Labels each from name_train & name_test)\n    # \n    db=sio.loadmat(nume+'_train.mat')\n    Samples=db['Samples'].astype('float32')\n    x_train=Samples.T\n    Labels=db['Labels'].astype('float32')\n    y_train=-1+Labels.T[:,0]\n    db=sio.loadmat(nume+'_test.mat')\n    Samples=db['Samples'].astype('float32')\n    x_test=Samples.T\n    Labels=-1+db['Labels'].astype('float32')\n    y_test=Labels.T[:,0]\n    x_train = x_train.astype('float32')\n    x_test = x_test.astype('float32')\n    return (x_train,x_test,y_train,y_test)\n\n#===============  TRAIN DATASET LOADING ==========================================\nfrom keras.datasets import mnist, cifar10, fashion_mnist\n\nif dataset=='mnist':\n    (x_train, y_train), (x_test, y_test) = mnist.load_data() # incarca date nescalate \nelif  dataset=='cifar10': \n    (x_train, y_train), (x_test, y_test) = cifar10.load_data() # incarca date nescalate \nelif dataset=='f-mnist':\n    (x_train, y_train), (x_test, y_test) =  fashion_mnist.load_data()\nelif dataset=='other': #load some local file of your choice (edit the name in the next lines)\n    nume='../input'+ifother\n    (x_train,x_test,y_train,y_test)=read_mat_data(nume)\n    \nif (np.ndim(x_train)==3):   # E.g.  MNIST or F-MNIST  \n    x_train=np.reshape(x_train, [np.shape(x_train)[0],np.shape(x_train)[1],np.shape(x_train)[2], 1]) \n    x_test=np.reshape(x_test, [np.shape(x_test)[0],np.shape(x_test)[1],np.shape(x_test)[2], 1] ) \n# place a  1 in the end to keep it compatible with kernel in conv2d \n# scaling in ([0,1])\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /=255 \n#=============================================================================\n# converts x_train, y_train into Samples Labels \nintrain=K.variable(x_train)\nSamples_=K.batch_flatten(intrain)  # aici se aplica direct datele de intrare \nSamples=(K.eval(Samples_)).T \nLabels=(y_train.T+1).astype('int8')\nif (np.ndim(Labels)<2): \n    Labels=np.reshape(Labels,[1,np.shape(Labels)[0]])\nclase=np.max(Labels)\n#================= TRAIN ELM =====================================================\nt1 = ti.time()\ninW, outW = elmTrain_fix(Samples, np.transpose(Labels), nr_neuroni, C, tip, nb_in)\ntrun = ti.time()-t1\nprint(\" training time: %f seconds\" %trun)\n# ==============  Quantify the output layer ======================================\nQout=-1+pow(2,nb_out-1)\nif nb_out>0:\n     O=np.max(np.abs(outW))\n     outW=np.round(outW*(1/O)*Qout)\n#================= TEST (VALIDATION) DATASET LOADING \nintest=K.variable(x_test)\nSamples_=K.batch_flatten(intest)  # aici se aplica direct datele de intrare \nSamples=(K.eval(Samples_)).T\nLabels=(y_test.T+1).astype('int8')\nif (np.ndim(Labels)<2):\n    Labels=np.reshape(Labels,[1,np.shape(Labels)[0]])  # acopera cazul MNIST \n\nn=Samples.shape[0]\nN=Samples.shape[1]\n#====================== VALIDATION PHASE (+ Accuracy evaluation) =================\nt1 = ti.time()\nscores = elmPredict_optim(Samples, inW, outW, tip)\ntrun = ti.time()-t1\nprint( \" prediction time: %f seconds\" %trun)\n\n# CONFUSION MATRIX computation ==================================\nConf=np.zeros((clase,clase),dtype='int16')\nfor i in range(N):\n    # gasire pozitie clasa prezisa \n    ix=np.where(scores[:,i]==np.max(scores[:,i]))\n    ixx=np.array(ix)\n    pred=int(ixx[0,0])\n    actual=Labels[0,i]-1\n    Conf[actual,pred]+=1\naccuracy=100.0*np.sum(np.diag(Conf))/np.sum(np.sum(Conf))\nprint(\"Confusion matrix is: \")\nprint(Conf)\nprint(\"Accuracy is: %f\" %accuracy)\nprint( \"Number of hidden neurons: %d\" %nr_neuroni)\nprint( \"Hidden nonlinearity (0=sigmoid; 1=linsat; 2=Relu; 3 - ABS; 4- multiquadric): %d\" %tip)\nK.clear_session() \n#====================================================================================   ","execution_count":7,"outputs":[{"output_type":"stream","text":"KERAS ACTIVE\nLLL - Less neurons than training samples\n training time: 7.904217 seconds\n prediction time: 0.869411 seconds\nConfusion matrix is: \n[[831   1  20  28   5   5  97   0  13   0]\n [  3 968   1  17   3   1   5   0   2   0]\n [ 14   0 801  12 102   0  68   0   3   0]\n [ 21   3  15 895  30   0  30   0   5   1]\n [  1   0  95  34 804   2  60   0   4   0]\n [  0   0   0   1   0 959   0  28   1  11]\n [139   1  90  28  73   3 648   0  18   0]\n [  0   0   0   0   0  13   0 954   0  33]\n [  1   0   4   4   3   3   8   4 973   0]\n [  0   0   0   0   0   5   1  27   0 967]]\nAccuracy is: 88.000000\nNumber of hidden neurons: 6000\nHidden nonlinearity (0=sigmoid; 1=linsat; 2=Relu; 3 - ABS; 4- multiquadric): 3\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}